{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "from skimage.transform import resize\n",
    "from netCDF4 import Dataset\n",
    "from pyrosm import OSM, get_data\n",
    "\n",
    "def reproject_sentinel_to_utm(sentinel_band_file, utm_zone, bands=[\"B02\", \"B03\", \"B04\", \"B08\"]):\n",
    "    ds = xr.load_dataset(sentinel_band_file)\n",
    "    band_data = ds[bands]\n",
    "    \n",
    "    utm_crs = f\"EPSG:{32600 + utm_zone}\"  # Assuming Northern Hemisphere\n",
    "    if 'crs' not in band_data.attrs:\n",
    "        band_data = band_data.rio.write_crs(utm_crs, inplace=True)\n",
    "    else:\n",
    "        band_data = band_data.rio.reproject(utm_crs)\n",
    "    \n",
    "    return band_data\n",
    "\n",
    "def plot_overlay(sentinel_band_file, jpeg_path, bands, time):\n",
    "    ds = sentinel_band_file\n",
    "    band_data = ds[bands].to_array(dim=\"bands\")\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(8, 8))\n",
    "    band_data[{\"t\": time}].plot.imshow(ax=ax, vmin=0, vmax=3500)\n",
    "    plt.title(f\"Sentinel-2 Natural Color Composite ({', '.join(bands)})\")\n",
    "    plt.show()\n",
    "    \n",
    "    # Read and plot the JPEG image\n",
    "    img = mpimg.imread(jpeg_path)\n",
    "    \n",
    "    # Get the extent of the Sentinel-2 data\n",
    "    xmin, ymin, xmax, ymax = band_data.rio.bounds()\n",
    "    \n",
    "    # Overlay the JPEG image with the correct extent\n",
    "    fig, ax = plt.subplots(figsize=(10, 8))\n",
    "    band_data[{\"t\": time}].plot.imshow(ax=ax, vmin=0, vmax=2000, cmap='gray')\n",
    "    ax.imshow(img, extent=[xmin, xmax, ymin, ymax], alpha=0.6)\n",
    "    \n",
    "    plt.title(\"Overlay of Sentinel-2 Band and JPEG Image\")\n",
    "    plt.xlabel(\"x coordinate of projection [m]\")\n",
    "    plt.ylabel(\"y coordinate of projection [m]\")\n",
    "    plt.show()\n",
    "\n",
    "def detect_clouds(patch_data, blue_threshold=2000, nir_threshold=5000, max_high_pixels=750):\n",
    "    blue_band = patch_data[0]  # B02\n",
    "    nir_band = patch_data[3]   # B08\n",
    "    \n",
    "    # Identify unique pixels that are high-value in either band\n",
    "    high_value_pixels = np.logical_or(blue_band > blue_threshold, nir_band > nir_threshold)\n",
    "    \n",
    "    # Count the number of unique high-value pixels\n",
    "    high_value_pixels_count = np.sum(high_value_pixels)\n",
    "    \n",
    "    # Debug information\n",
    "    print(f\"Number of unique high-value pixels: {high_value_pixels_count}\")\n",
    "    \n",
    "    # Return True if the number of high-valued pixels exceeds the threshold\n",
    "    return high_value_pixels_count > max_high_pixels\n",
    "\n",
    "def create_and_save_patches(band_array, jpeg_path, patch_size, stride, output_dir, bands, time_index=1):\n",
    "    # Read the JPEG image\n",
    "    img = mpimg.imread(jpeg_path)\n",
    "    cloud_counter = 0\n",
    "    # Debug: Print shape of the band array and the JPEG image\n",
    "    print(f\"Shape of band_array: {band_array.shape}\")\n",
    "    print(f\"Shape of JPEG image: {img.shape}\")\n",
    "    \n",
    "    # Resize JPEG image to match the Sentinel-2 data dimensions\n",
    "    sentinel_height, sentinel_width = band_array.shape[2], band_array.shape[3]\n",
    "    resized_img = resize(img, (sentinel_height, sentinel_width, img.shape[2]), anti_aliasing=True)\n",
    "    \n",
    "    # Debug: Print shape of the resized JPEG image\n",
    "    print(f\"Shape of resized JPEG image: {resized_img.shape}\")\n",
    "    \n",
    "    # Calculate the number of patches in x and y directions ensuring no overlap\n",
    "    num_patches_x = max(1, int((sentinel_width - patch_size) / patch_size) + 1)\n",
    "    num_patches_y = max(1, int((sentinel_height - patch_size) / patch_size) + 1)\n",
    "    \n",
    "    # Debug: Print number of patches in X and Y directions\n",
    "    print(f\"Total patches to be created: {num_patches_x * num_patches_y}\")\n",
    "    print(f\"Number of patches in X direction: {num_patches_x}\")\n",
    "    print(f\"Number of patches in Y direction: {num_patches_y}\")\n",
    "    \n",
    "    # Create output directory if it doesn't exist\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Iterate over the grid to create patches\n",
    "    patch_index = 0\n",
    "    for i in range(num_patches_y):\n",
    "        for j in range(num_patches_x):\n",
    "            # Extract the patch from the Sentinel-2 data\n",
    "            sentinel_patch = band_array[\n",
    "                :,  # Keep all bands\n",
    "                time_index,  # Use the specified time index\n",
    "                i * patch_size:(i + 1) * patch_size,\n",
    "                j * patch_size:(j + 1) * patch_size\n",
    "            ]\n",
    "            \n",
    "            # Flip the Sentinel-2 patch horizontally\n",
    "            # sentinel_patch = np.flip(sentinel_patch, axis=1)\n",
    "            print(f\"checking cloudy patch at ({i}, {j})\")\n",
    "            if detect_clouds(sentinel_patch):\n",
    "                cloud_counter+=1\n",
    "                print(f\"Skipping cloudy patch at ({i}, {j})\")\n",
    "                continue\n",
    "            # Ensure the patch is of the correct size\n",
    "            if sentinel_patch.shape[1] != patch_size or sentinel_patch.shape[2] != patch_size:\n",
    "                print(f\"Skipping Sentinel patch at ({i}, {j}) due to incorrect size.\")\n",
    "                continue\n",
    "            \n",
    "            # Extract the corresponding patch from the resized JPEG image\n",
    "            jpeg_patch = resized_img[\n",
    "                i * patch_size:(i + 1) * patch_size,\n",
    "                j * patch_size:(j + 1) * patch_size,\n",
    "                :\n",
    "            ]\n",
    "            \n",
    "            # Ensure the patch is of the correct size\n",
    "            if jpeg_patch.shape[0] != patch_size or jpeg_patch.shape[1] != patch_size:\n",
    "                print(f\"Skipping JPEG patch at ({i}, {j}) due to incorrect size.\")\n",
    "                continue\n",
    "            \n",
    "            # Save the Sentinel-2 patch as a NetCDF file\n",
    "            sentinel_patch_file = os.path.join(output_dir, f'sentinel_patch_{patch_index}normal.nc')\n",
    "            sentinel_patch_ds = xr.Dataset(\n",
    "                {\n",
    "                    'B02': (['y', 'x'], sentinel_patch[0]),\n",
    "                    'B03': (['y', 'x'], sentinel_patch[1]),\n",
    "                    'B04': (['y', 'x'], sentinel_patch[2]),\n",
    "                    'B08': (['y', 'x'], sentinel_patch[3])\n",
    "                }\n",
    "            )\n",
    "            sentinel_patch_ds.to_netcdf(sentinel_patch_file)\n",
    "            \n",
    "            # Save the JPEG patch as a JPEG file\n",
    "            jpeg_patch_file = os.path.join(output_dir, f'jpeg_patch_{patch_index}normal.jpg')\n",
    "            plt.imsave(jpeg_patch_file, jpeg_patch)\n",
    "            \n",
    "            print(f\"Patch {patch_index}:\")\n",
    "            print(f\"  Sentinel patch file: {sentinel_patch_file}\")\n",
    "            print(f\"  JPEG patch file: {jpeg_patch_file}\")\n",
    "            \n",
    "            patch_index += 1\n",
    "    \n",
    "    print(f'Created {patch_index} patches and saved to {output_dir}')\n",
    "    print(f\"cloud patches removed {cloud_counter}\")\n",
    "\n",
    "# Function to visualize and overlay patches from Sentinel-2 and JPEG images\n",
    "def visualize_and_overlay_patches(patch_folder, num_patches, bands):\n",
    "    patch_files = [f for f in os.listdir(patch_folder) if f.startswith('sentinel_patch_') and f.endswith('.nc')]\n",
    "    jpeg_files = [f for f in os.listdir(patch_folder) if f.startswith('jpeg_patch_') and f.endswith('.jpg')]\n",
    "    patch_files.sort()\n",
    "    jpeg_files.sort()\n",
    "\n",
    "    for i in range(min(num_patches, len(patch_files))):\n",
    "        patch_file = patch_files[i]\n",
    "        jpeg_file = jpeg_files[i]\n",
    "        \n",
    "        patch_file_path = os.path.join(patch_folder, patch_file)\n",
    "        jpeg_file_path = os.path.join(patch_folder, jpeg_file)\n",
    "\n",
    "        # Read NetCDF patch\n",
    "        with Dataset(patch_file_path, 'r') as ds:\n",
    "            patch_data = np.array([ds[band][:] for band in bands])\n",
    "            band_data = xr.DataArray(patch_data, dims=(\"bands\", \"y\", \"x\"), coords={\"bands\": bands})\n",
    "\n",
    "        # Ensure the patch is 3D for RGB display\n",
    "        if patch_data.shape[0] >= 3:\n",
    "            sentinel_rgb_patch = np.stack([patch_data[2], patch_data[1], patch_data[0]], axis=-1)\n",
    "        else:\n",
    "            raise ValueError(\"Sentinel patch must have at least three bands for RGB visualization.\")\n",
    "\n",
    "        # Load JPEG patch\n",
    "        jpeg_patch = mpimg.imread(jpeg_file_path)\n",
    "        \n",
    "        # Plot Sentinel-2 patch using band_data\n",
    "        fig, ax = plt.subplots(figsize=(8, 8))\n",
    "        # Plot overlay of Sentinel-2 patch and JPEG patch\n",
    "        ax.imshow(sentinel_rgb_patch, vmin=0, vmax=3500)\n",
    "        ax.imshow(jpeg_patch, alpha=0.6)\n",
    "        ax.set_title(f'Overlay Patch {i}')\n",
    "        ax.axis('off')\n",
    "\n",
    "        #Part two\n",
    "        fig, ax = plt.subplots(figsize=(8, 8))\n",
    "        band_data.plot.imshow(ax=ax, vmin=0, vmax=3500)\n",
    "        plt.title(f\"Sentinel-2 Natural Color Composite ({', '.join(bands)})\")\n",
    "        plt.show()\n",
    "            # Read and plot the JPEG image\n",
    "        img = mpimg.imread(jpeg_file_path)\n",
    "        \n",
    "        # Get the extent of the Sentinel-2 data\n",
    "        \n",
    "        # Overlay the JPEG image with the correct extent\n",
    "        fig, ax = plt.subplots(figsize=(10, 8))\n",
    "        band_data.plot.imshow(ax=ax, vmin=0, vmax=2000)\n",
    "        ax.imshow(img, alpha=0.6)\n",
    "        plt.title(\"Overlay of Sentinel-2 Band and JPEG Image\")\n",
    "        plt.show()\n",
    "\n",
    "cities = {\n",
    "    \"muenchen\": {\"lat\": [48.0800, 48.1900], \"lon\": [11.5000, 11.6700], \"osm\": \"muenchen.osm.pbf\", \"utm_zone\": 32, \"sentinel_file\": \"muenchen_sentinel.nc\", \"time\": 0},\n",
    "    \"rotterdam\": {\"lat\": [51.8750, 51.9700], \"lon\": [4.4200, 4.5400], \"osm\": \"rotterdam.osm.pbf\", \"utm_zone\": 31, \"sentinel_file\": \"rotterdam_sentinel.nc\", \"time\": 0},\n",
    "    \"duesseldorf\": {\"lat\": [51.1646, 51.3086], \"lon\": [6.6847, 6.8785], \"osm\": \"duesseldorf.osm.pbf\", \"utm_zone\": 32, \"sentinel_file\": \"duesseldorf_sentinel.nc\", \"time\": 0},\n",
    "    \"prag\": {\"lat\": [49.9900, 50.1100], \"lon\": [14.3500, 14.5400], \"osm\": \"prag.osm.pbf\", \"utm_zone\": 33, \"sentinel_file\": \"prag_sentinel.nc\", \"time\": 0},\n",
    "    \"greater_london\": {\"lat\": [51.4400, 51.5800], \"lon\": [-0.2000, 0.0200], \"osm\": \"greater_london.osm.pbf\", \"utm_zone\": 30, \"sentinel_file\": \"greater_london_sentinel.nc\", \"time\": 2},\n",
    "    \"dublin\": {\"lat\": [53.3244, 53.4273], \"lon\": [-6.3874, -6.1071], \"osm\": \"dublin.osm.pbf\", \"utm_zone\": 29, \"sentinel_file\": \"dublin_sentinel.nc\", \"time\": 1},\n",
    "    \"frankfurt\": {\"lat\": [50.0250, 50.2100], \"lon\": [8.5200, 8.7500], \"osm\": \"frankfurt.osm.pbf\", \"utm_zone\": 32, \"sentinel_file\": \"frankfurt_sentinel.nc\", \"time\": 2},\n",
    "    \"madrid\": {\"lat\": [40.3125, 40.6437], \"lon\": [-3.8890, -3.5556], \"osm\": \"madrid.osm.pbf\", \"utm_zone\": 30, \"sentinel_file\": \"madrid_sentinel.nc\", \"time\": -1},\n",
    "    \"bruessel\": {\"lat\": [50.7968, 50.9101], \"lon\": [4.3054, 4.4317], \"osm\": \"bruessel.osm.pbf\", \"utm_zone\": 31, \"sentinel_file\": \"bruessel_sentinel.nc\", \"time\": 0},\n",
    "    \"berlin\": {\"lat\": [52.454927,52.574409], \"lon\": [13.294333, 13.500205], \"osm\": \"berlin.osm.pbf\", \"utm_zone\": 33, \"sentinel_file\": \"berlin_sentinel.nc\", \"time\": 1},\n",
    "    \"magdeburg\": {\"lat\": [52.0762, 52.1985], \"lon\": [11.5430, 11.6760], \"osm\": \"magdeburg.osm.pbf\", \"utm_zone\": 32, \"sentinel_file\": \"magdeburg_sentinel.nc\", \"time\" :0},\n",
    "    \"bremerhaven\": {\"lat\": [53.4978, 53.6008], \"lon\": [8.5142, 8.6552], \"osm\": \"bremerhaven.osm.pbf\", \"utm_zone\": 32, \"sentinel_file\": \"bremerhaven_sentinel.nc\", \"time\" :1},\n",
    "    \"nuernberg\": {\"lat\": [49.3955, 49.4904], \"lon\": [11.0023, 11.1445], \"osm\": \"nuernberg.osm.pbf\", \"utm_zone\": 32, \"sentinel_file\": \"nuernberg_sentinel.nc\", \"time\" :0},\n",
    "    \"erfurt\": {\"lat\": [50.9300, 51.0100], \"lon\": [11.0100, 11.1100], \"osm\": \"erfurt.osm.pbf\", \"utm_zone\": 32, \"sentinel_file\": \"erfurt_sentinel.nc\", \"time\" :0},\n",
    "    \"rostock\": {\"lat\": [54.0647, 54.1237], \"lon\": [12.0735, 12.1616], \"osm\": \"rostock.osm.pbf\", \"utm_zone\": 33, \"sentinel_file\": \"rostock_sentinel.nc\", \"time\" :0},\n",
    "    \"chemnitz\": {\"lat\": [50.7830, 50.8750], \"lon\": [12.8830, 12.9550], \"osm\": \"chemnitz.osm.pbf\", \"utm_zone\": 33, \"sentinel_file\": \"chemnitz_sentinel.nc\", \"time\" :3},\n",
    "    \"potsdam\": {\"lat\": [52.3567, 52.4348], \"lon\": [13.0142, 13.1208], \"osm\": \"potsdam.osm.pbf\", \"utm_zone\": 33, \"sentinel_file\": \"potsdam_sentinel.nc\", \"time\" :1},\n",
    "    \"bonn\": {\"lat\": [50.6540, 50.7640], \"lon\": [7.0525, 7.1650], \"osm\": \"bonn.osm.pbf\", \"utm_zone\": 32, \"sentinel_file\": \"bonn_sentinel.nc\", \"time\" :0},\n",
    "    \"duisburg\": {\"lat\": [51.3650, 51.5150], \"lon\": [6.6840, 6.8460], \"osm\": \"duisburg.osm.pbf\", \"utm_zone\": 32, \"sentinel_file\": \"duisburg_sentinel.nc\", \"time\" :0},\n",
    "    \"osnabrueck\": {\"lat\": [52.2450, 52.3050], \"lon\": [7.9500, 8.0800], \"osm\": \"osnabrueck.osm.pbf\", \"utm_zone\": 32, \"sentinel_file\": \"osnabrueck_sentinel.nc\", \"time\" :3},\n",
    "}\n",
    "patch_size = 64\n",
    "stride = 64\n",
    "bands = ['B02', 'B03', 'B04', 'B08']\n",
    "num_patches_to_visualize = 5\n",
    "for city, data in cities.items():\n",
    "    lat = data[\"lat\"]\n",
    "    lon = data[\"lon\"]\n",
    "    time = data[\"time\"]\n",
    "    bbox = [lon[0], lat[0], lon[1], lat[1]]\n",
    "    print(f\"Reprojecting Sentinel-2 data for {city}\")\n",
    "    sentinel_ds = reproject_sentinel_to_utm(data[\"sentinel_file\"], data[\"utm_zone\"], bands=bands)\n",
    "    \n",
    "    output_path = f\"./output/{city}_patches\"  # Ensure this directory exists\n",
    "\n",
    "    # Process OSM data and save as JPEG\n",
    "    print(f\"Processing OSM data for {city}\")\n",
    "    #process_osm_data(city, data[\"osm\"], sentinel_ds, bbox, output_path)\n",
    "    \n",
    "    # Create patches for Sentinel-2 and JPEG images\n",
    "    jpeg_path = os.path.join(output_path, f'{city}_buildings.jpg')\n",
    "    band_array = sentinel_ds[bands].to_array(dim=\"bands\").values\n",
    "    \n",
    "    plot_overlay(sentinel_ds, jpeg_path, bands,time)\n",
    "    #print(f\"Creating and saving patches for {city}\")\n",
    "    create_and_save_patches(band_array, jpeg_path, patch_size, stride, output_path, bands,time)\n",
    "\n",
    "    # Visualize and overlay patches\n",
    "    #print(f\"Visualizing and overlaying patches for {city}\")\n",
    "    visualize_and_overlay_patches(output_path, num_patches_to_visualize, bands)\n",
    "\n",
    "    print(f\"Creating and saving patches for {city}\")\n",
    "    #create_and_save_patches(band_array, jpeg_path, patch_size, stride, output_path, bands, time)\n",
    "\n",
    "    # Display cloud-free patches\n",
    "    print(f\"Displaying cloud-free patches for {city}\")\n",
    "    #display_cloud_free_patches(output_path, num_patches_to_visualize, bands)\n",
    "    #plot_overlay(sentinel_ds, jpeg_path, bands)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from netCDF4 import Dataset\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def create_tensors_from_patches(patch_folder, bands, aug_type = ''):\n",
    "    nc_files = sorted([f for f in os.listdir(patch_folder) if f.startswith('sentinel_patch_') and aug_type in f and f.endswith('.nc')])\n",
    "    jpeg_files = sorted([f for f in os.listdir(patch_folder) if f.startswith('jpeg_patch_') and aug_type in f and f.endswith('.jpg')])\n",
    "    \n",
    "    print(f\"Found {len(nc_files)} NC files and {len(jpeg_files)} JPEG files\")\n",
    "\n",
    "    def load_sentinel_patch(file_path):\n",
    "        try:\n",
    "            with Dataset(file_path, 'r') as ds:\n",
    "                patch_data = np.array([ds[band][:] for band in bands])\n",
    "            return patch_data\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading Sentinel patch {file_path}: {str(e)}\")\n",
    "            return None\n",
    "\n",
    "    def load_jpeg_patch(file_path):\n",
    "        try:\n",
    "            with Image.open(file_path) as img:\n",
    "                img_array = np.array(img.convert('L')).astype(np.float32) / 255.0\n",
    "            return img_array\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading JPEG patch {file_path}: {str(e)}\")\n",
    "            return None\n",
    "\n",
    "    sentinel_patches = []\n",
    "    jpeg_patches = []\n",
    "\n",
    "    for nc_file, jpeg_file in zip(nc_files, jpeg_files):\n",
    "        sentinel_patch = load_sentinel_patch(os.path.join(patch_folder, nc_file))\n",
    "        jpeg_patch = load_jpeg_patch(os.path.join(patch_folder, jpeg_file))\n",
    "        \n",
    "        if sentinel_patch is not None and jpeg_patch is not None:\n",
    "            sentinel_patches.append(sentinel_patch)\n",
    "            jpeg_patches.append(jpeg_patch)\n",
    "\n",
    "    if sentinel_patches and jpeg_patches:\n",
    "        # Reshape input tensor to channels last format\n",
    "        input_tensor = tf.convert_to_tensor(np.transpose(np.array(sentinel_patches), (0, 2, 3, 1)), dtype=tf.float32)\n",
    "        \n",
    "        # Target tensor should not include a channel dimension\n",
    "        target_tensor = tf.convert_to_tensor(np.array(jpeg_patches), dtype=tf.float32)\n",
    "        \n",
    "        print(f\"Input tensor shape: {input_tensor.shape}\")\n",
    "        print(f\"Target tensor shape: {target_tensor.shape}\")\n",
    "        \n",
    "        return input_tensor, target_tensor\n",
    "    else:\n",
    "        print(\"Failed to create tensors: No valid patches found\")\n",
    "        return None, None\n",
    "\n",
    "def visualize_tensors(input_tensor, target_tensor, bands, num_samples=5):\n",
    "    num_samples = min(num_samples, input_tensor.shape[0])\n",
    "    \n",
    "    for i in range(num_samples):\n",
    "        fig, axes = plt.subplots(1, len(bands) + 1, figsize=(20, 4))\n",
    "        fig.suptitle(f\"Sample {i+1}\")\n",
    "\n",
    "        for j, band in enumerate(bands):\n",
    "            axes[j].imshow(input_tensor[i, :, :, j], cmap='viridis')\n",
    "            axes[j].set_title(f\"Band {band}\")\n",
    "            axes[j].axis('off')\n",
    "\n",
    "        # Binary mask (no need to squeeze)\n",
    "        axes[len(bands)].imshow(target_tensor[i], cmap='binary')\n",
    "        axes[len(bands)].set_title(\"Binary Building Mask\")\n",
    "        axes[len(bands)].axis('off')\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        # Print mask values statistics\n",
    "        mask_values = target_tensor[i].numpy()\n",
    "        print(f\"Sample {i+1} Binary Building Mask Statistics:\")\n",
    "        print(f\"  Min value: {np.min(mask_values):.4f}\")\n",
    "        print(f\"  Max value: {np.max(mask_values):.4f}\")\n",
    "        print(f\"  Mean value: {np.mean(mask_values):.4f}\")\n",
    "        print(f\"  Unique values: {np.unique(mask_values)}\")\n",
    "        print()\n",
    "\n",
    "\n",
    "def process_city(city_name, city_data, bands):\n",
    "    print(f\"\\nProcessing {city_name}\")\n",
    "    patch_folder = f\"./output/{city_name}_patches\"\n",
    "    \n",
    "    if not os.path.exists(patch_folder):\n",
    "        print(f\"Patch folder not found for {city_name}. Skipping.\")\n",
    "        return None, None\n",
    "\n",
    "    input_tensor, target_tensor = create_tensors_from_patches(patch_folder, bands)\n",
    "    \n",
    "    if input_tensor is not None and target_tensor is not None:\n",
    "        print(f\"Tensors created for {city_name}\")\n",
    "        print(f\"Input tensor shape: {input_tensor.shape}\")\n",
    "        print(f\"Target tensor shape: {target_tensor.shape}\")\n",
    "        \n",
    "        # Visualize the first few samples\n",
    "        visualize_tensors(input_tensor, target_tensor, bands)\n",
    "    else:\n",
    "        print(f\"Failed to create tensors for {city_name}\")\n",
    "    \n",
    "    return input_tensor, target_tensor\n",
    "\n",
    "# City data\n",
    "cities = {\n",
    "    \"muenchen\": {\"lat\": [48.0800, 48.1900], \"lon\": [11.5000, 11.6700], \"osm\": \"muenchen.osm.pbf\", \"utm_zone\": 32, \"sentinel_file\": \"muenchen_sentinel.nc\", \"time\": 0},\n",
    "    \"rotterdam\": {\"lat\": [51.8750, 51.9700], \"lon\": [4.4200, 4.5400], \"osm\": \"rotterdam.osm.pbf\", \"utm_zone\": 31, \"sentinel_file\": \"rotterdam_sentinel.nc\", \"time\": 0},\n",
    "    \"duesseldorf\": {\"lat\": [51.1646, 51.3086], \"lon\": [6.6847, 6.8785], \"osm\": \"duesseldorf.osm.pbf\", \"utm_zone\": 32, \"sentinel_file\": \"duesseldorf_sentinel.nc\", \"time\": 0},\n",
    "    \"prag\": {\"lat\": [49.9900, 50.1100], \"lon\": [14.3500, 14.5400], \"osm\": \"prag.osm.pbf\", \"utm_zone\": 33, \"sentinel_file\": \"prag_sentinel.nc\", \"time\": 0},\n",
    "    \"greater_london\": {\"lat\": [51.4400, 51.5800], \"lon\": [-0.2000, 0.0200], \"osm\": \"greater_london.osm.pbf\", \"utm_zone\": 30, \"sentinel_file\": \"greater_london_sentinel.nc\", \"time\": 2},\n",
    "    \"dublin\": {\"lat\": [53.3244, 53.4273], \"lon\": [-6.3874, -6.1071], \"osm\": \"dublin.osm.pbf\", \"utm_zone\": 29, \"sentinel_file\": \"dublin_sentinel.nc\", \"time\": 1},\n",
    "    \"frankfurt\": {\"lat\": [50.0250, 50.2100], \"lon\": [8.5200, 8.7500], \"osm\": \"frankfurt.osm.pbf\", \"utm_zone\": 32, \"sentinel_file\": \"frankfurt_sentinel.nc\", \"time\": 2},\n",
    "    \"madrid\": {\"lat\": [40.3125, 40.6437], \"lon\": [-3.8890, -3.5556], \"osm\": \"madrid.osm.pbf\", \"utm_zone\": 30, \"sentinel_file\": \"madrid_sentinel.nc\", \"time\": -1},\n",
    "    \"bruessel\": {\"lat\": [50.7968, 50.9101], \"lon\": [4.3054, 4.4317], \"osm\": \"bruessel.osm.pbf\", \"utm_zone\": 31, \"sentinel_file\": \"bruessel_sentinel.nc\", \"time\": 0},\n",
    "    #\"singapore\": {\"lat\": [1.2378, 1.4707], \"lon\": [103.6017, 104.0123], \"osm\": \"singapore.osm.pbf\", \"utm_zone\": 48, \"sentinel_file\": \"singapore_sentinel.nc\", \"time\": 0},\n",
    "    \"berlin\": {\"lat\": [52.454927,52.574409], \"lon\": [13.294333, 13.500205], \"osm\": \"berlin.osm.pbf\", \"utm_zone\": 33, \"sentinel_file\": \"berlin_sentinel.nc\", \"time\": 1},\n",
    "    \"magdeburg\": {\"lat\": [52.0762, 52.1985], \"lon\": [11.5430, 11.6760], \"osm\": \"magdeburg.osm.pbf\", \"utm_zone\": 32, \"sentinel_file\": \"magdeburg_sentinel.nc\", \"time\" :0},\n",
    "    \"bremerhaven\": {\"lat\": [53.4978, 53.6008], \"lon\": [8.5142, 8.6552], \"osm\": \"bremerhaven.osm.pbf\", \"utm_zone\": 32, \"sentinel_file\": \"bremerhaven_sentinel.nc\", \"time\" :1},\n",
    "    \"nuernberg\": {\"lat\": [49.3955, 49.4904], \"lon\": [11.0023, 11.1445], \"osm\": \"nuernberg.osm.pbf\", \"utm_zone\": 32, \"sentinel_file\": \"nuernberg_sentinel.nc\", \"time\" :0},\n",
    "    \"erfurt\": {\"lat\": [50.9300, 51.0100], \"lon\": [11.0100, 11.1100], \"osm\": \"erfurt.osm.pbf\", \"utm_zone\": 32, \"sentinel_file\": \"erfurt_sentinel.nc\", \"time\" :0},\n",
    "    \"rostock\": {\"lat\": [54.0647, 54.1237], \"lon\": [12.0735, 12.1616], \"osm\": \"rostock.osm.pbf\", \"utm_zone\": 33, \"sentinel_file\": \"rostock_sentinel.nc\", \"time\" :0},\n",
    "    \"chemnitz\": {\"lat\": [50.7830, 50.8750], \"lon\": [12.8830, 12.9550], \"osm\": \"chemnitz.osm.pbf\", \"utm_zone\": 33, \"sentinel_file\": \"chemnitz_sentinel.nc\", \"time\" :3},\n",
    "    \"potsdam\": {\"lat\": [52.3567, 52.4348], \"lon\": [13.0142, 13.1208], \"osm\": \"potsdam.osm.pbf\", \"utm_zone\": 33, \"sentinel_file\": \"potsdam_sentinel.nc\", \"time\" :1},\n",
    "    \"bonn\": {\"lat\": [50.6540, 50.7640], \"lon\": [7.0525, 7.1650], \"osm\": \"bonn.osm.pbf\", \"utm_zone\": 32, \"sentinel_file\": \"bonn_sentinel.nc\", \"time\" :0},\n",
    "    \"duisburg\": {\"lat\": [51.3650, 51.5150], \"lon\": [6.6840, 6.8460], \"osm\": \"duisburg.osm.pbf\", \"utm_zone\": 32, \"sentinel_file\": \"duisburg_sentinel.nc\", \"time\" :0},\n",
    "    \"osnabrueck\": {\"lat\": [52.2450, 52.3050], \"lon\": [7.9500, 8.0800], \"osm\": \"osnabrueck.osm.pbf\", \"utm_zone\": 32, \"sentinel_file\": \"osnabrueck_sentinel.nc\", \"time\" :3},\n",
    "}\n",
    "\n",
    "def create_binary_mask(target_tensor, threshold=0.08):\n",
    "    \"\"\"\n",
    "    Convert the inverted floating-point mask to a binary mask.\n",
    "    \n",
    "    Args:\n",
    "    target_tensor: Tensor of shape (N, H, W) with inverted mask values\n",
    "    threshold: Value above which a pixel is considered a building (default: 0.06)\n",
    "    \n",
    "    Returns:\n",
    "    Binary mask tensor of shape (N, H, W)\n",
    "    \"\"\"\n",
    "    # Invert the mask if it hasn't been inverted yet\n",
    "    inverted_mask = 1 - target_tensor\n",
    "    \n",
    "    # Convert to binary mask\n",
    "    binary_mask = tf.cast(inverted_mask > threshold, tf.float32)\n",
    "    \n",
    "    return binary_mask\n",
    "\n",
    "# Modify the filter_empty_or_full_masks function\n",
    "def filter_empty_or_full_masks(input_tensor, target_tensor, min_threshold=0.01, max_threshold=0.99):\n",
    "    # Create binary mask\n",
    "    binary_mask = create_binary_mask(target_tensor)\n",
    "    \n",
    "    # Calculate the fraction of building pixels in each mask\n",
    "    mask_fraction = tf.reduce_mean(binary_mask, axis=[1, 2])\n",
    "    \n",
    "    # Create a boolean mask for samples to keep\n",
    "    keep_mask =  (mask_fraction > min_threshold) & (mask_fraction < max_threshold)\n",
    "    \n",
    "    # Apply the mask to both input tensor and binary mask\n",
    "    filtered_input = tf.boolean_mask(input_tensor, keep_mask, axis=0)\n",
    "    filtered_target = tf.boolean_mask(binary_mask, keep_mask, axis=0)\n",
    "    \n",
    "    print(f\"Kept {tf.reduce_sum(tf.cast(keep_mask, tf.int32))} out of {keep_mask.shape[0]} samples\")\n",
    "    \n",
    "    return filtered_input, filtered_target\n",
    "\n",
    "# Function to process city data with filtering\n",
    "def process_city_with_filtering(city_name, city_data, bands,aug_type, min_threshold=0.01, max_threshold=0.99):\n",
    "    print(f\"\\nProcessing {city_name}\")\n",
    "    patch_folder = f\"./output/{city_name}_patches\"\n",
    "    \n",
    "    if not os.path.exists(patch_folder):\n",
    "        print(f\"Patch folder not found for {city_name}. Skipping.\")\n",
    "        return None, None\n",
    "\n",
    "    input_tensor, target_tensor = create_tensors_from_patches(patch_folder, bands, aug_type = aug_type)\n",
    "    \n",
    "    if input_tensor is not None and target_tensor is not None:\n",
    "        print(\"Before filtering:\")\n",
    "        print(f\"  Input tensor shape: {input_tensor.shape}\")\n",
    "        print(f\"  Target tensor shape: {target_tensor.shape}\")\n",
    "        \n",
    "        # Apply filtering\n",
    "        filtered_input, filtered_target = filter_empty_or_full_masks(input_tensor, target_tensor, min_threshold, max_threshold)\n",
    "        \n",
    "        print(\"After filtering:\")\n",
    "        print(f\"  Filtered input tensor shape: {filtered_input.shape}\")\n",
    "        print(f\"  Filtered target tensor shape: {filtered_target.shape}\")\n",
    "        \n",
    "        # Visualize the first few samples of filtered data\n",
    "        visualize_tensors(filtered_input, filtered_target, bands)\n",
    "    else:\n",
    "        print(f\"Failed to create tensors for {city_name}\")\n",
    "        return None, None\n",
    "    \n",
    "    return filtered_input, filtered_target\n",
    "\n",
    "# Process all cities\n",
    "bands = ['B02', 'B03', 'B04', 'B08']\n",
    "all_city_tensors = {}\n",
    "def invert_mask(mask):\n",
    "    return 1 - mask\n",
    "\n",
    "\n",
    "for city_name, city_data in cities.items():\n",
    "    #input_tensor, target_tensor = process_city(city_name, city_data, bands)\n",
    "    input_tensor_normal, target_tensor_normal = process_city_with_filtering(city_name, city_data, bands,min_threshold=0.0, aug_type='normal')\n",
    "    input_tensor_reflect, target_tensor_reflect = process_city_with_filtering(city_name, city_data, bands,min_threshold=0.0, aug_type='reflect')\n",
    "    #input_tensor, target_tensor = process_city_with_filtering(city_name, city_data, bands,min_threshold=0.0, aug_type='shear')\n",
    "    #input_tensor, target_tensor = process_city_with_filtering(city_name, city_data, bands,min_threshold=0.0, aug_type='rotate')\n",
    "    input_tensor = tf.concat([input_tensor_normal, input_tensor_reflect], axis=0)  \n",
    "    target_tensor = tf.concat([target_tensor_normal, target_tensor_reflect], axis=0)  \n",
    "    # Apply this to your target tensor\n",
    "    inverted_target_tensor = invert_mask(target_tensor)\n",
    "    if input_tensor is not None and target_tensor is not None:\n",
    "        all_city_tensors[city_name] = {\n",
    "            \"input\": input_tensor,\n",
    "            \"target\": inverted_target_tensor\n",
    "        }\n",
    "\n",
    "# Print summary of processed cities\n",
    "print(\"\\nSummary of processed cities:\")\n",
    "for city_name, tensors in all_city_tensors.items():\n",
    "    print(f\"{city_name}:\")\n",
    "    print(f\"  Input tensor shape: {tensors['input'].shape}\")\n",
    "    print(f\"  Target tensor shape: {tensors['target'].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "# Debug statement to check if all_city_tensors is loaded correctly\n",
    "print(\"Loaded city tensors keys:\", all_city_tensors.keys())\n",
    "\n",
    "# Assuming all_city_tensors is already created from the previous step\n",
    "\n",
    "# Separate Berlin (test set) from other cities\n",
    "test_input = all_city_tensors['berlin']['input']\n",
    "test_target = all_city_tensors['berlin']['target']\n",
    "print(f\"Berlin test set shapes - Input: {test_input.shape}, Target: {test_target.shape}\")\n",
    "\n",
    "# Combine other cities for training and validation\n",
    "train_val_input = []\n",
    "train_val_target = []\n",
    "for city, tensors in all_city_tensors.items():\n",
    "    if city != 'berlin':\n",
    "        train_val_input.append(tensors['input'])\n",
    "        train_val_target.append(tensors['target'])\n",
    "        print(f\"Adding data from city: {city}, Input shape: {tensors['input'].shape}, Target shape: {tensors['target'].shape}\")\n",
    "\n",
    "train_val_input = np.concatenate(train_val_input, axis=0)\n",
    "train_val_target = np.concatenate(train_val_target, axis=0)\n",
    "print(f\"Combined train/val shapes - Input: {train_val_input.shape}, Target: {train_val_target.shape}\")\n",
    "\n",
    "# Split into training and validation sets\n",
    "train_input, val_input, train_target, val_target = train_test_split(\n",
    "    train_val_input, train_val_target, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "print(f\"Training set shapes - Input: {train_input.shape}, Target: {train_target.shape}\")\n",
    "print(f\"Validation set shapes - Input: {val_input.shape}, Target: {val_target.shape}\")\n",
    "\n",
    "# Define the CNN model\n",
    "def create_model(input_shape):\n",
    "    p = 'same'\n",
    "    model = models.Sequential([\n",
    "        layers.Conv2D(32, (3, 3), activation='relu', padding=p, input_shape=input_shape),\n",
    "        layers.Conv2D(64, (3, 3), activation='relu', padding=p),\n",
    "        layers.Conv2D(128, (3, 3), activation='relu', padding=p),\n",
    "        layers.Conv2D(1, (1, 1), padding=p)\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "# Create the model\n",
    "input_shape = train_input.shape[1:]  # (H, W, C)\n",
    "print(f\"Model input shape: {input_shape}\")\n",
    "model = create_model(input_shape)\n",
    "model.summary()  # Print model summary\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model with early stopping\n",
    "early_stopping = EarlyStopping(patience=5, restore_best_weights=True)\n",
    "history = model.fit(\n",
    "    train_input, train_target,\n",
    "    validation_data=(val_input, val_target),\n",
    "    epochs=50,\n",
    "    batch_size=32,\n",
    "    callbacks=[early_stopping]\n",
    ")\n",
    "\n",
    "# Evaluate the model on test set (Berlin)\n",
    "test_loss, test_accuracy = model.evaluate(test_input, test_target)\n",
    "print(f\"Test accuracy: {test_accuracy}\")\n",
    "\n",
    "# Plot training history\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.title('Model Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Model Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Hyperparameter tuning\n",
    "def create_tuned_model(input_shape, l2_reg=0.01, learning_rate=0.001):\n",
    "    p = 'same'\n",
    "    model = models.Sequential([\n",
    "        layers.Conv2D(32, (3, 3), activation='relu', padding=p, input_shape=input_shape, kernel_regularizer=l2(l2_reg)),\n",
    "        layers.Conv2D(64, (3, 3), activation='relu', padding=p, kernel_regularizer=l2(l2_reg)),\n",
    "        layers.Conv2D(128, (3, 3), activation='relu', padding=p, kernel_regularizer=l2(l2_reg)),\n",
    "        layers.Conv2D(1, (1, 1), padding=p, kernel_regularizer=l2(l2_reg))\n",
    "    ])\n",
    "    model.compile(optimizer=Adam(learning_rate=learning_rate), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Hyperparameter grid\n",
    "l2_regs = [0.001, 0.01, 0.1]\n",
    "learning_rates = [0.0001, 0.001, 0.01]\n",
    "\n",
    "best_val_accuracy = 0\n",
    "best_model = None\n",
    "best_params = None\n",
    "\n",
    "for l2_reg in l2_regs:\n",
    "    for lr in learning_rates:\n",
    "        print(f\"Training with L2 reg: {l2_reg}, Learning rate: {lr}\")\n",
    "        model = create_tuned_model(input_shape, l2_reg, lr)\n",
    "        history = model.fit(\n",
    "            train_input, train_target,\n",
    "            validation_data=(val_input, val_target),\n",
    "            epochs=30,\n",
    "            batch_size=32,\n",
    "            callbacks=[EarlyStopping(patience=5, restore_best_weights=True)],\n",
    "            verbose=0\n",
    "        )\n",
    "        val_accuracy = max(history.history['val_accuracy'])\n",
    "        print(f\"Validation accuracy for L2 reg {l2_reg}, Learning rate {lr}: {val_accuracy}\")\n",
    "        if val_accuracy > best_val_accuracy:\n",
    "            best_val_accuracy = val_accuracy\n",
    "            best_model = model\n",
    "            best_params = {'l2_reg': l2_reg, 'learning_rate': lr}\n",
    "\n",
    "print(\"Best hyperparameters:\", best_params)\n",
    "print(\"Best validation accuracy:\", best_val_accuracy)\n",
    "\n",
    "# Evaluate best model on test set (Berlin)\n",
    "test_loss, test_accuracy = best_model.evaluate(test_input, test_target)\n",
    "print(f\"Test accuracy with best model: {test_accuracy}\")\n",
    "\n",
    "# U-Net implementation\n",
    "def unet_model(input_shape):\n",
    "    inputs = layers.Input(input_shape)\n",
    "    # Encoder\n",
    "    conv1 = layers.Conv2D(64, 3, activation='relu', padding='same')(inputs)\n",
    "    pool1 = layers.MaxPooling2D(pool_size=(2, 2))(conv1)\n",
    "\n",
    "    conv2 = layers.Conv2D(128, 3, activation='relu', padding='same')(pool1)\n",
    "    pool2 = layers.MaxPooling2D(pool_size=(2, 2))(conv2)\n",
    "\n",
    "    # Bridge\n",
    "    conv3 = layers.Conv2D(256, 3, activation='relu', padding='same')(pool2)\n",
    "\n",
    "    # Decoder\n",
    "    up4 = layers.UpSampling2D(size=(2, 2))(conv3)\n",
    "    up4 = layers.concatenate([up4, conv2])\n",
    "    conv4 = layers.Conv2D(128, 3, activation='relu', padding='same')(up4)\n",
    "\n",
    "    up5 = layers.UpSampling2D(size=(2, 2))(conv4)\n",
    "    up5 = layers.concatenate([up5, conv1])\n",
    "    conv5 = layers.Conv2D(64, 3, activation='relu', padding='same')(up5)\n",
    "\n",
    "    outputs = layers.Conv2D(1, 1, activation='sigmoid')(conv5)\n",
    "\n",
    "    model = models.Model(inputs=inputs, outputs=outputs)\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Train U-Net model\n",
    "unet = unet_model(input_shape)\n",
    "unet_history = unet.fit(\n",
    "    train_input, train_target,\n",
    "    validation_data=(val_input, val_target),\n",
    "    epochs=50,\n",
    "    batch_size=32,\n",
    "    callbacks=[EarlyStopping(patience=5, restore_best_weights=True)]\n",
    ")\n",
    "\n",
    "# Evaluate U-Net on test set (Berlin)\n",
    "unet_test_loss, unet_test_accuracy = unet.evaluate(test_input, test_target)\n",
    "print(f\"U-Net Test accuracy: {unet_test_accuracy}\")\n",
    "\n",
    "# Compare CNN and U-Net performance\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['accuracy'], label='CNN Training')\n",
    "plt.plot(history.history['val_accuracy'], label='CNN Validation')\n",
    "plt.plot(unet_history.history['accuracy'], label='U-Net Training')\n",
    "plt.plot(unet_history.history['val_accuracy'], label='U-Net Validation')\n",
    "plt.title('Model Accuracy Comparison')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['loss'], label='CNN Training')\n",
    "plt.plot(history.history['val_loss'], label='CNN Validation')\n",
    "plt.plot(unet_history.history['loss'], label='U-Net Training')\n",
    "plt.plot(unet_history.history['val_loss'], label='U-Net Validation')\n",
    "plt.title('Model Loss Comparison')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10 (py310)",
   "language": "python",
   "name": "py310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
